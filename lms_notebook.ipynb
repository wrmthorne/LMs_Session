{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Language Modeling in Python\n",
    "\n",
    "This notebook will cover the basics of using near state-of-the-art models in natural language processing. The library allows users to submit open source models  (on [huggingface](https://huggingface.co/)) with trained weights for others to reuse. Models on this site are of the transformer architecture - hence the library is called \"transformers\".\n",
    "\n",
    "Language models are used as the foundation for further tasks in NLP as they capture the concept of language which is valuable for use in tasks such as named entity recognition (identifying people, places, or organisations), sentiment analysis (determing whether text is positive or negative) or other text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-31 13:17:59--  https://raw.githubusercontent.com/wrmthorne/LMs_Session/main/requirements.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49 [text/plain]\n",
      "Saving to: ‘requirements.txt.1’\n",
      "\n",
      "requirements.txt.1  100%[===================>]      49  --.-KB/s    in 0s      \n",
      "\n",
      "2023-03-31 13:18:00 (9.65 MB/s) - ‘requirements.txt.1’ saved [49/49]\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "Collecting bertviz\n",
      "  Using cached bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
      "Collecting jupyterlab\n",
      "  Downloading jupyterlab-3.6.3-py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ipywidgets\n",
      "  Using cached ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting triton==2.0.0\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 1)) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 1)) (0.37.1)\n",
      "Collecting lit\n",
      "  Using cached lit-16.0.0.tar.gz (144 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Using cached cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/liam/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (23.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers->-r requirements.txt (line 2)) (2.25.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/liam/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (1.24.1)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.103-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jupyter-core in /home/liam/.local/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 4)) (5.1.3)\n",
      "Collecting notebook<7\n",
      "  Using cached notebook-6.5.3-py3-none-any.whl (529 kB)\n",
      "Collecting tomli\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting jupyter-server-ydoc~=0.8.0\n",
      "  Using cached jupyter_server_ydoc-0.8.0-py3-none-any.whl (11 kB)\n",
      "Collecting jupyterlab-server~=2.19\n",
      "  Downloading jupyterlab_server-2.22.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nbclassic\n",
      "  Downloading nbclassic-0.5.4-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: ipython in /home/liam/.local/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 4)) (8.8.0)\n",
      "Collecting jupyter-ydoc~=0.2.3\n",
      "  Using cached jupyter_ydoc-0.2.3-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: tornado>=6.1.0 in /home/liam/.local/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 4)) (6.2)\n",
      "Collecting jupyter-server<3,>=1.16.0\n",
      "  Using cached jupyter_server-2.5.0-py3-none-any.whl (366 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Using cached jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "Collecting widgetsnbextension~=4.0.7\n",
      "  Using cached widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/liam/.local/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 5)) (6.20.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/liam/.local/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 5)) (5.8.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/liam/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (1.6.5)\n",
      "Requirement already satisfied: nest-asyncio in /home/liam/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (1.5.6)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/liam/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/liam/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (7.4.9)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/liam/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (25.0.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/liam/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (0.1.6)\n",
      "Requirement already satisfied: psutil in /home/liam/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (5.9.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /home/liam/.local/lib/python3.10/site-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (0.6.2)\n",
      "Requirement already satisfied: backcall in /home/liam/.local/lib/python3.10/site-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/liam/.local/lib/python3.10/site-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /home/liam/.local/lib/python3.10/site-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/liam/.local/lib/python3.10/site-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (2.14.0)\n",
      "Requirement already satisfied: decorator in /home/liam/.local/lib/python3.10/site-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in /home/liam/.local/lib/python3.10/site-packages (from ipython->jupyterlab->-r requirements.txt (line 4)) (3.0.36)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.0.1)\n",
      "Collecting jupyter-events>=0.4.0\n",
      "  Using cached jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
      "Collecting send2trash\n",
      "  Using cached Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting jupyter-server-terminals\n",
      "  Using cached jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.16.0-py3-none-any.whl (122 kB)\n",
      "Collecting argon2-cffi\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Using cached terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Using cached nbformat-5.8.0-py3-none-any.whl (77 kB)\n",
      "Collecting anyio>=3.1.0\n",
      "  Using cached anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "Collecting nbconvert>=6.4.4\n",
      "  Using cached nbconvert-7.2.10-py3-none-any.whl (275 kB)\n",
      "Collecting websocket-client\n",
      "  Using cached websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/liam/.local/lib/python3.10/site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 4)) (2.6.2)\n",
      "Collecting jupyter-server-fileid<1,>=0.6.0\n",
      "  Using cached jupyter_server_fileid-0.8.0-py3-none-any.whl (15 kB)\n",
      "Collecting ypy-websocket<0.9.0,>=0.8.2\n",
      "  Using cached ypy_websocket-0.8.4-py3-none-any.whl (10 kB)\n",
      "Collecting y-py<0.6.0,>=0.5.3\n",
      "  Using cached y_py-0.5.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting jsonschema>=4.17.3\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting json5>=0.9.0\n",
      "  Using cached json5-0.9.11-py2.py3-none-any.whl (19 kB)\n",
      "Collecting babel>=2.10\n",
      "  Using cached Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "Collecting ipython-genutils\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting notebook-shim>=0.1.0\n",
      "  Using cached notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (1.26.5)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.3)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.103\n",
      "  Downloading botocore-1.29.103-py3-none-any.whl (10.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/liam/.local/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.103->boto3->bertviz->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/liam/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython->jupyterlab->-r requirements.txt (line 4)) (0.8.3)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Using cached pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: entrypoints in /home/liam/.local/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 5)) (0.4)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting mistune<3,>=2.0.3\n",
      "  Using cached mistune-2.0.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting tinycss2\n",
      "  Using cached tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.0-py3-none-any.whl (132 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Using cached nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "Collecting fastjsonschema\n",
      "  Using cached fastjsonschema-2.16.3-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: wcwidth in /home/liam/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython->jupyterlab->-r requirements.txt (line 4)) (0.2.6)\n",
      "Requirement already satisfied: ptyprocess in /usr/lib/python3/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 4)) (0.7.0)\n",
      "Collecting aiofiles<23,>=22.1.0\n",
      "  Using cached aiofiles-22.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting aiosqlite<1,>=0.17.0\n",
      "  Using cached aiosqlite-0.18.0-py3-none-any.whl (15 kB)\n",
      "Collecting ypy-websocket<0.9.0,>=0.8.2\n",
      "  Using cached ypy_websocket-0.8.3-py3-none-any.whl (10 kB)\n",
      "  Using cached ypy_websocket-0.8.2-py3-none-any.whl (10 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/liam/.local/lib/python3.10/site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/liam/.local/lib/python3.10/site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/liam/.local/lib/python3.10/site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython->jupyterlab->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting fqdn\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting isoduration\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting uri-template\n",
      "  Using cached uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting jsonpointer>1.13\n",
      "  Using cached jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting webcolors>=1.11\n",
      "  Using cached webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Using cached cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.4-py3-none-any.whl (37 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting arrow>=0.15.0\n",
      "  Using cached arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93602 sha256=2ce2efcabdd0f5fc6daf846486ea865188cc7af94ce0a605289028ee92520a21\n",
      "  Stored in directory: /home/liam/.cache/pip/wheels/a3/f5/8f/b11227e816563ac08fce423c6e617f05f5fe1a18d9bcfb2375\n",
      "Successfully built lit\n",
      "Installing collected packages: y-py, webencodings, tokenizers, sentencepiece, send2trash, mpmath, mistune, lit, json5, ipython-genutils, fastjsonschema, cmake, widgetsnbextension, websocket-client, webcolors, uri-template, typing-extensions, tqdm, tomli, tinycss2, terminado, sympy, soupsieve, sniffio, rfc3986-validator, rfc3339-validator, regex, python-json-logger, pyrsistent, pycparser, prometheus-client, pandocfilters, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, jupyterlab-widgets, jupyterlab-pygments, jupyter-ydoc, jsonpointer, jmespath, jinja2, fqdn, filelock, defusedxml, charset-normalizer, bleach, babel, attrs, aiosqlite, aiofiles, ypy-websocket, requests, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jupyter-server-terminals, jsonschema, cffi, botocore, beautifulsoup4, arrow, anyio, s3transfer, nbformat, isoduration, huggingface-hub, argon2-cffi-bindings, transformers, nbclient, boto3, argon2-cffi, nbconvert, jupyter-events, ipywidgets, jupyter-server, notebook-shim, jupyterlab-server, jupyter-server-fileid, nbclassic, jupyter-server-ydoc, notebook, jupyterlab, triton, torch, bertviz\n",
      "Successfully installed aiofiles-22.1.0 aiosqlite-0.18.0 anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 attrs-22.2.0 babel-2.12.1 beautifulsoup4-4.12.0 bertviz-1.4.0 bleach-6.0.0 boto3-1.26.103 botocore-1.29.103 cffi-1.15.1 charset-normalizer-3.1.0 cmake-3.26.1 defusedxml-0.7.1 fastjsonschema-2.16.3 filelock-3.10.7 fqdn-1.5.1 huggingface-hub-0.13.3 ipython-genutils-0.2.0 ipywidgets-8.0.6 isoduration-20.11.0 jinja2-3.1.2 jmespath-1.0.1 json5-0.9.11 jsonpointer-2.3 jsonschema-4.17.3 jupyter-events-0.6.3 jupyter-server-2.5.0 jupyter-server-fileid-0.8.0 jupyter-server-terminals-0.4.4 jupyter-server-ydoc-0.8.0 jupyter-ydoc-0.2.3 jupyterlab-3.6.3 jupyterlab-pygments-0.2.2 jupyterlab-server-2.22.0 jupyterlab-widgets-3.0.7 lit-16.0.0 mistune-2.0.5 mpmath-1.3.0 nbclassic-0.5.4 nbclient-0.7.2 nbconvert-7.2.10 nbformat-5.8.0 networkx-3.0 notebook-6.5.3 notebook-shim-0.2.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandocfilters-1.5.0 prometheus-client-0.16.0 pycparser-2.21 pyrsistent-0.19.3 python-json-logger-2.0.7 regex-2023.3.23 requests-2.28.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 s3transfer-0.6.0 send2trash-1.8.0 sentencepiece-0.1.97 sniffio-1.3.0 soupsieve-2.4 sympy-1.11.1 terminado-0.17.1 tinycss2-1.2.1 tokenizers-0.13.2 tomli-2.0.1 torch-2.0.0 tqdm-4.65.0 transformers-4.27.4 triton-2.0.0 typing-extensions-4.5.0 uri-template-1.2.0 webcolors-1.13 webencodings-0.5.1 websocket-client-1.5.1 widgetsnbextension-4.0.7 y-py-0.5.9 ypy-websocket-0.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installs dependencies\n",
    "!wget https://raw.githubusercontent.com/wrmthorne/LMs_Session/main/requirements.txt\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline, utils, T5ForConditionalGeneration\n",
    "from bertviz import model_view, head_view\n",
    "from bertviz.neuron_view import show\n",
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "\n",
    "utils.logging.set_verbosity_error()\n",
    "\n",
    "model_type = 'bert'\n",
    "model_version = 'bert-base-cased'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising\n",
    "\n",
    "Computer models cannot understand or perform mathematical operations on words so they need to be represented in a machine readable format. Tradiationally, this was done by giving each unique word a number (index). This has a problem. To represent all words that have and will ever exist, you need infinitely many indices which is impossible to compute.\n",
    "\n",
    "Instead, one method called word-piece was developed which builds a vocabulary with sub-word \"tokens\" which are then converted to indices. The algorithm starts with the letters of the alphabet, numbers and puctuation and finds how to combine characters into larger chunks based on how many times they appear together in text. This limits the vocabulary size to an upper limit while being able to produce every word (and misspelling) in the language.\n",
    "\n",
    "e.g. the word \"WANDISCO\" may be split into \"WA ##ND ##IS ##CO\" (## indicates that this is stuck to the end of another token). We aren't likely to see the full word as many times as other words so it won't be in the vocabulary but the subcomponents may appear in other words too. The algorithm favours creating short chunks because they appear more frequently on average.\n",
    "\n",
    "The transformers library provides us with pretrained tokenisers to use for their matching model (the tokeniser must match that used to train the model or you will get random behaviour). If you wanted to train your own model, you can also train a tokeniser to go with it. For now we will use the BERT model as an example.\n",
    "\n",
    "BERT also adds two extra tokens to your input string. `[CLS]` which is used for tasks that build on top of the language model such as predicting missing words and `[SEP]` which is used to indicate the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "\n",
    "# Feel free to change the test string\n",
    "test_string = 'WANDISCO'\n",
    "\n",
    "token_ids = tokenizer.encode(test_string)\n",
    "print(f'IDS: {token_ids}')\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f'Tokens: {tokens}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The language model itself doesn't do much. It is effectively a big memory unit for how language functions, ontop of which \"heads\" are applied. A head is a new network which is design specifically to solve a task. One such task - which is how BERT is trained - is to predict a missing word in a sequence. This is called `masked language modelling`. We can find out what type of company BERT thinks WANDISCO is by using the `[MASK]` token on the word we want it to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask', model=model_version)\n",
    "\n",
    "test_string = 'WANDISCO is a [MASK] company'\n",
    "unmasker(test_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "The model is only as good as the data it is trained on and the model likely has inherent bias. Most models are trained on internet data or books. Therefore, the views of the model will reflect that of their training data. For example, we can compare what BERT predicts as an occupation for men and women (trained on [wikipedia](https://www.wikipedia.org/) and [BookCorpus](https://paperswithcode.com/dataset/bookcorpus)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_preds = unmasker('The man worked as a [MASK].')\n",
    "female_preds = unmasker('The woman worked as a [MASK].')\n",
    "\n",
    "print(f'Male predictions: {[pred[\"token_str\"] for pred in male_preds]}')\n",
    "print(f'Female predictions: {[pred[\"token_str\"] for pred in female_preds]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The transformer architecture makes use of \"attention\" which is a way of learning which words a specific word is focusing on. e.g.\n",
    "\n",
    "\"The rabbit ran up the hill and the fox followed\"\n",
    "\n",
    "The word \"rabbit\" would likely pay more attention to the words \"The\" as it is its article and \"ran\" is the verb associated with the rabbit. It is less likely to pay attention to the \"the\" preceding \"fox\" as it is the article for the fox rather than the rabbit.\n",
    "\n",
    "Transformer models like BERT make use of many attention blocks (layers) and within each block, use many attention heads. Each of these blocks and heads should ideally look at a different concept. Some of these may be easily interpretable by humans (such as the example above) but some are abstract concepts, only understandable by the model. If you are interested, the research area of [mechanistic interpretability](https://www.neelnanda.io/mechanistic-interpretability) hopes to understand what is happening at a neuron level in the attention mechanism.\n",
    "\n",
    "Inspecting Layer 2 Head 3 in BERT, we can see that it has learned that each word should look to the next word in the sequence. Layer 6 Head 4 learns that words are part of the same sentence and the assignment of articles to their nouns (e.g. 'the' -> 'rabbit'). Have a look through the layers and heads and try and find any obvious properties learned in a particular head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = \"The rabbit ran up the hill and the fox followed\"\n",
    "\n",
    "bert_model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=False)\n",
    "show(bert_model, model_type, bert_tokenizer, sentence_a, layer=2, head=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an overview of what every head is doing in every layer by plotting them all. Many of them link the whole sequence to the `[SEP]` token and many others link to the `[CLS]` token. These tokens are called \"special tokens\" as they are used for extra functions beyond modelling language. As mentioned before, `[CLS]` is for classification and use in guiding the heads which extend the model and `[SEP]` is important for defining sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "\n",
    "inputs = tokenizer.encode(sentence_a, return_tensors='pt')\n",
    "outputs = model(inputs)\n",
    "attention = outputs[-1] \n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "\n",
    "model_view(attention, tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next cell, think about what would happen if we put two sentences in the sequence. We would have two `[SEP]` tokens because we have two sentences but only 1 `[CLS]` token because they are part of the same sequence. What do you think the attention diagrams will look like?\n",
    "\n",
    "Run the cell and find out if it is what you expected? Try to think about why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_b = \"The rabbit ran up the hill and the fox did not follow\"\n",
    "\n",
    "inputs = tokenizer.encode(sentence_a, sentence_b, return_tensors='pt')\n",
    "outputs = model(inputs)\n",
    "attention = outputs[-1] \n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "\n",
    "print(' '.join(tokens))\n",
    "model_view(attention, tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Architectures\n",
    "\n",
    "There are 3 main types of transformer architecture: encoder-decoder, encoder-only and decoder-only. [BERT](https://huggingface.co/bert-base-cased) is an encoder only model. It encodes the text into a machine understandable format called a latent space. This latent space is accessible in BERT through the CLS token which allows another neural network to use its language modelling to perform further tasks (AKA downstream tasks).\n",
    "\n",
    "[GPT](https://huggingface.co/gpt2) is a decoder only model. It decodes the input text directly into the output text.\n",
    "\n",
    "[BARD](https://bard.google.com/) and [T5](https://huggingface.co/t5-base) are encoder-decoder models. The encoder translates the text into the latent space and this latent space is read by the decoder. This makes these models especially useful for translation. Encoder-only and decoder-only models only make use of one type of attention called self-attention which is where the sequence looks only at other terms in the sequence. Encoder-decoder models make use of self-attention in the encoder and the decoder separately, but have a second type of attention called cross-attention which allows the decoder to inspect the input sequence.\n",
    "\n",
    "To generate text, a decoder is needed, hence why BERT cannot be used for text generation.\n",
    "\n",
    "Let's have a look at T5 (encoder-decoder) for text generation. The authors of T5 claimed that any task can be reformulated as a text to text task and uses a system called prefix and prompt. The model is trained to perform many tasks including translation, question & answering, and classification tasks. The task you want it to perform is defined by a prefix e.g. \"translate English to German: \" and the content you want the task performed on is the prompt e.g. \"I flew to Germany\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small', output_attentions=True)\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained('t5-small', model_max_length=512)\n",
    "\n",
    "input = t5_tokenizer('translate English to German: I flew to Germany.', return_tensors='pt')\n",
    "output_ids = t5_model.generate(**input, max_length=512) # ** is used to unpack the dictionary\n",
    "\n",
    "output_tokens = t5_tokenizer.batch_decode(output_ids)\n",
    "output_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a go at some other prefixes and prompts to see what T5 can do.\n",
    "\n",
    "Some of the prefixes are:\n",
    "- translate English to German\n",
    "- summarize (`NOTE`: American spelling)\n",
    "- generate question\n",
    "- answer question\n",
    "- predict sentiment\n",
    "- binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'summarize:'\n",
    "prefix = 'This is a long story about the rabbit and the fox. The rabbit ran up the hill to escape the fox. One time the fox followed. The other time, it did not.'\n",
    "\n",
    "input = t5_tokenizer(prompt + ' ' + prefix, return_tensors='pt')\n",
    "output_ids = t5_model.generate(**input, max_length=512)\n",
    "\n",
    "output_tokens = t5_tokenizer.batch_decode(output_ids)\n",
    "output_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the attention diagrams for T5. Because T5 is an encoder-decoder model, we have the encoder attentions, the decoder attentions and a series of cross attentions which map between the attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_ids = t5_tokenizer(\"translate English to German: I flew to Germany.\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
    "decoder_input_ids = t5_tokenizer(text_target=\"Ich flog nach Deutschland.\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
    "\n",
    "outputs = t5_model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "encoder_text = t5_tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n",
    "decoder_text = t5_tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
    "\n",
    "model_view(\n",
    "    encoder_attention=outputs.encoder_attentions,\n",
    "    decoder_attention=outputs.decoder_attentions,\n",
    "    cross_attention=outputs.cross_attentions,\n",
    "    encoder_tokens=encoder_text,\n",
    "    decoder_tokens=decoder_text\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lms_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
